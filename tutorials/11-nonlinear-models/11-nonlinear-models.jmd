---
title: Non-Linear Models
permalink: /:collection/:name/
redirect_from: tutorials/11-linearregression/
---

Non-linear models are flexible and powerful, but require more care in both selection of priors and in model specification than (generalized) linear models. Since Turing models are written in Julia itself it is quite simple and natural to formulate non-linear models. Below we work through a few non-linear modeling scenarios, based on similar [tutorials](https://paul-buerkner.github.io/brms/articles/brms_nonlinear.html) and [blog posts](https://thinkinator.com/2016/02/17/brms-0-8-adds-non-linear-regression) originally written for the R package brms.

## Set Up

We begin by importing all the necessary libraries.


```julia
# Import Turing and Distributions.
using Turing, Distributions

# Import DataFrames.
using DataFrames

# Import MCMCChains, Plots, and StatPlots for visualizations and diagnostics.
using MCMCChains, Plots, StatsPlots

# Functionality for evaluating the model predictions.
using Distances

# Set a seed for reproducibility.
using Random
Random.seed!(0)

# Hide the progress prompt while sampling.
Turing.setprogress!(false);
```

## Non-linear regression of the Michaelis-Menten kinetics curve

Wayne Folta, in his [blog post](https://thinkinator.com/2016/02/17/brms-0-8-adds-non-linear-regression) showing non-linear modeling in brms, used data from the Michaelis-Menten kinetics curve (from the [Wikipedia page for non-linear regression](https://en.wikipedia.org/wiki/Nonlinear_regression)).

```julia
data = DataFrame(R = Float64[0, 0.04, 0.07, 0.11, 0.165, 0.225, 0.27, 0.31, 0.315],
                 S = Float64[0,   30,   70,  140,   300,   600, 1200, 2600,  3800]);

# Show the first six rows of the dataset.
first(data, 6)
```

According to the Wikipedia article, the formula is: 

$$
\frac{{\beta}_1 * \boldsymbol{x}}{{\beta}_2 + \boldsymbol{x}} 
$$

and the way to implement this in Turing is:

```julia
michaelis_menten(b1, b2, s) = (b1 * s) / (b2 + s)
# Bayesian non-linear regression, Michaelis-Menten kinetics.
@model function nonlinear_regression_MM(R, S)
    # Set the priors on our coefficients.
    b1 ~ truncated(Gamma(1.5, 0.8),   0.001, Inf)
    # With the same prior as in the brms example we converge to a bad estimate of b2:
    #b2 ~ truncated(Gamma(1.5, 0.002), 0.001, Inf) # Not clear why Turing's NUTS sampler behaves differently from the brms/Stan one here...
    # If problem with the above prior this one with more mass up high seems more stable:
    b2 ~ truncated(Normal(500, 200), 1.0, Inf)

    # Set variance prior. Default in brms is a Normal(0, 1)??
    σ ~ truncated(Normal(0, 1), 0, Inf)

    # Calculate all the mu terms.
    for i in eachindex(R)
        r = michaelis_menten(b1, b2, S[i]) # (b1 * S[i]) / (b2 + S[i]) # Michaelis-Menten kinetics model
        R[i] ~ Normal(r, sqrt(σ))
    end
end
```

We used the same priors for the coefficients as Wayne did and a non-informative prior on the variance. Note that in Turing there is nothing special you need to indicate you are doing non-linear modeling. You simply use a non-linear form when relating the independent variables in your data (here S) and your coefficients (here b1 and b2). This is in contrast to the R package brms where there is an explicit argument you need to set to indicate non-linear modeling.

With our model specified, we can call the sampler. We will use the Dynamic NUTS sampler.

```julia
model = nonlinear_regression_MM(data.R, data.S)
chain = sample(model, NUTS(0.65), 4_000);
```

Possibly you will see some warnings during sampling telling you that "the current proposal will be rejected due to numerical error(s)". This is not uncommon in non-linear modeling since the flexible form can allow for very large (or small) values being produced. (TODO: Explain this better and in more detail)

As a visual check to confirm that our coefficients have converged, we show the densities and trace plots for our parameters using the `plot` functionality.


```julia
plot(chain)
```

It looks like each of our parameters have converged. We can check our numerical estimates and quantiles using:

```julia
chain
```

We can see that we got reasonably large effective sample size (ess) and Rhat close to 1.0. Let's now use the posterior distribution of our coefficients and predict with them. We create a function that calculates the median and the 95% credible intervals when predicting with the posterior samples (while skipping the first halfs of the chains).

```julia
# Make predictions given the parameter values of the chain. Skip the initial half.
function prediction(f::Function, chain, paramnames, X, numskip = div(length(chain), 2))
    paramvals = map(n -> getproperty(get(chain, n), n), paramnames)
    medvals, qlows, qhighs = Float64[], Float64[], Float64[]
    for x in X
        preds = vec(f.(paramvals..., x))
        l, m, h = quantile(preds, [0.025, 0.50, 0.975])
        push!(qlows, l)
        push!(medvals, m)
        push!(qhighs, h)
    end
    return medvals, qlows, qhighs
end

median_predictions, qlows, qhighs = prediction(michaelis_menten, chain, [:b1, :b2], data.S)
```

Let's use the mean square distance (msd in the Distances package) to evaluate our predictions:

```julia
msd(data.R, median_predictions)
```

We can see that the mean squared loss i very small (about 2e-5 in my runs).